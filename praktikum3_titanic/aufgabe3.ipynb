{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('titanic.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isnull().sum()/df.shape[0]*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check duplicates\n",
    "df.duplicated().sum() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Löschen der Spalte 'Cabin' aus dem DataFrame weil 77% missing values\n",
    "df.drop('Cabin', axis=1, inplace=True)\n",
    "#Age abrunden\n",
    "df['Age'] = np.floor(df['Age'])\n",
    "#missing values treatment\n",
    "df['Age'].fillna(df['Age'].median(), inplace=True) # ist's sinvoll?\n",
    "df['Age'] = df['Age'].astype(int)\n",
    "df['Embarked'] = df['Embarked'].fillna(df['Embarked'].mode()[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check missing values\n",
    "df.isnull().sum()/df.shape[0]*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "#extrahiere new column 'Titel',das könnte wichtig für datenanalyse sein?\n",
    "df[\"Title\"] = df[\"Name\"].apply(lambda x: x.split(\",\")[1].split(\".\")[0].strip())\n",
    "df[\"Name\"] = df[\"Name\"].apply(lambda x: ' '.join([word for word in x.split() if not word.endswith('.')]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Version 1 (not using)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import base64\n",
    "import io\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import StratifiedKFold, cross_validate, train_test_split, cross_val_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier, plot_tree\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, f1_score, make_scorer, precision_score, recall_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.utils import resample\n",
    "import plotly.graph_objects as go\n",
    "import plotly.express as px\n",
    "from dash import Dash, dcc, html\n",
    "\n",
    "# 1. DATA PREPARATION\n",
    "df = pd.read_csv('titanic.csv')\n",
    "\n",
    "# Clean the data\n",
    "# These columns (PassengerId, Name, Ticket, Cabin) are not useful for predicting survival. They either contain unique identifiers or non-informative text data that do not contribute to the model's predictive power.\n",
    "df = df.drop(columns=['PassengerId', 'Name', 'Ticket', 'Cabin'], errors='ignore')\n",
    "# The 'Age' column has missing values. Using the median to fill these gaps is a common practice because it is robust to outliers and provides a central tendency measure.\n",
    "df['Age'] = np.floor(df['Age'])\n",
    "df['Age'].fillna(df['Age'].median(), inplace=True)\n",
    "# The 'Embarked' column has missing values. Filling them with the most frequent value.\n",
    "df['Embarked'].fillna(df['Embarked'].mode()[0], inplace=True)\n",
    "df = pd.get_dummies(df, columns=['Sex', 'Embarked'], drop_first=True)\n",
    "# Features and target: To prepare the data for modeling, we need to separate the features (input variables) from the target (output variable).\n",
    "X = df.drop(columns=['Survived'])\n",
    "y = df['Survived']\n",
    "\n",
    "# Feature Scaling\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "\n",
    "# Models\n",
    "models = {\n",
    "    \"Logistic Regression\": LogisticRegression(max_iter=1000),\n",
    "    \"Decision Tree\": DecisionTreeClassifier(random_state=42),\n",
    "    \"KNN (k=3)\": KNeighborsClassifier(n_neighbors=3)\n",
    "}\n",
    "\n",
    "# Cross-validation and Bootstrap methods\n",
    "def cross_validate_method(model, X, y):\n",
    "    cv = StratifiedKFold(n_splits=10, shuffle=True, random_state=42)\n",
    "    scoring = {\n",
    "        'accuracy': 'accuracy',\n",
    "        'precision': make_scorer(precision_score, average='weighted'),\n",
    "        'recall': make_scorer(recall_score, average='weighted'),\n",
    "        'f1': make_scorer(f1_score, average='weighted')\n",
    "    }\n",
    "    scores = cross_validate(model, X, y, cv=cv, scoring=scoring, return_estimator=True)\n",
    "    \n",
    "    confusion_matrices = []\n",
    "    for estimator, (train_index, test_index) in zip(scores['estimator'], cv.split(X, y)):\n",
    "        y_pred = estimator.predict(X[test_index])\n",
    "        cm = confusion_matrix(y[test_index], y_pred)\n",
    "        confusion_matrices.append(cm)\n",
    "    \n",
    "    mean_confusion_matrix = np.mean(confusion_matrices, axis=0)\n",
    "    \n",
    "    return {\n",
    "        'accuracy': scores['test_accuracy'].mean(),\n",
    "        'precision': scores['test_precision'].mean(),\n",
    "        'recall': scores['test_recall'].mean(),\n",
    "        'f1': scores['test_f1'].mean(),\n",
    "        'confusion_matrix': mean_confusion_matrix\n",
    "    }\n",
    "\n",
    "def bootstrap_632_method(model, X, y):\n",
    "    \"\"\"Perform Bootstrap .632 method with extended metrics.\"\"\"\n",
    "    train_metrics, test_metrics = [], []\n",
    "    confusion_matrices = []\n",
    "\n",
    "    for _ in range(100):  # Number of bootstrap iterations\n",
    "        # Bootstrap sampling\n",
    "        bootstrap_indices = np.random.choice(np.arange(len(X)), size=len(X), replace=True)\n",
    "        oob_mask = ~np.isin(np.arange(len(X)), bootstrap_indices)\n",
    "        \n",
    "        X_boot, y_boot = X[bootstrap_indices], y[bootstrap_indices]\n",
    "        X_oob, y_oob = X[oob_mask], y[oob_mask]\n",
    "\n",
    "        # Train the model on in-bag data\n",
    "        model.fit(X_boot, y_boot)\n",
    "        y_pred_train = model.predict(X_boot)\n",
    "\n",
    "        # Collect train metrics\n",
    "        train_metrics.append({\n",
    "            'accuracy': accuracy_score(y_boot, y_pred_train),\n",
    "            'precision': precision_score(y_boot, y_pred_train, average='weighted'),\n",
    "            'recall': recall_score(y_boot, y_pred_train, average='weighted'),\n",
    "            'f1': f1_score(y_boot, y_pred_train, average='weighted'),\n",
    "        })\n",
    "\n",
    "        # Collect test metrics if there are out-of-bag samples\n",
    "        if len(y_oob) > 0:\n",
    "            y_pred_oob = model.predict(X_oob)\n",
    "            test_metrics.append({\n",
    "                'accuracy': accuracy_score(y_oob, y_pred_oob),\n",
    "                'precision': precision_score(y_oob, y_pred_oob, average='weighted'),\n",
    "                'recall': recall_score(y_oob, y_pred_oob, average='weighted'),\n",
    "                'f1': f1_score(y_oob, y_pred_oob, average='weighted'),\n",
    "            })\n",
    "            confusion_matrices.append(confusion_matrix(y_oob, y_pred_oob))\n",
    "\n",
    "    # Aggregate train and test metrics\n",
    "    train_metrics_mean = {metric: np.mean([m[metric] for m in train_metrics]) for metric in train_metrics[0]}\n",
    "    test_metrics_mean = {metric: np.mean([m[metric] for m in test_metrics]) for metric in test_metrics[0]}\n",
    "\n",
    "    # Combine metrics using the .632 formula\n",
    "    combined_metrics = {metric: 0.368 * train_metrics_mean[metric] + 0.632 * test_metrics_mean[metric] for metric in train_metrics_mean}\n",
    "    mean_confusion_matrix = np.mean(confusion_matrices, axis=0) if confusion_matrices else None\n",
    "\n",
    "    combined_metrics['confusion_matrix'] = mean_confusion_matrix\n",
    "    return combined_metrics\n",
    "\n",
    "# Evaluate models\n",
    "metrics_results = {}\n",
    "confusion_matrices = {}\n",
    "crossval_results = {}\n",
    "bootstrap_results = {}\n",
    "\n",
    "\n",
    "for model_name, model in models.items():\n",
    "    # Cross-validation\n",
    "    crossval_metrics = cross_validate_method(model, X, y)\n",
    "    \n",
    "    # Bootstrap\n",
    "    bootstrap_metrics = bootstrap_632_method(model, X, y)\n",
    "    \n",
    "    # Save metrics\n",
    "    metrics_results[model_name] = {\n",
    "        'CrossVal Accuracy': crossval_metrics['accuracy'],\n",
    "        'CrossVal Precision': crossval_metrics['precision'],\n",
    "        'CrossVal Recall': crossval_metrics['recall'],\n",
    "        'CrossVal F1-Score': crossval_metrics['f1'],\n",
    "        'Bootstrap Accuracy': bootstrap_metrics['accuracy'],\n",
    "        'Bootstrap Precision': bootstrap_metrics['precision'],\n",
    "        'Bootstrap Recall': bootstrap_metrics['recall'],\n",
    "        'Bootstrap F1-Score': bootstrap_metrics['f1']\n",
    "    }\n",
    "    \n",
    "    # Save confusion matrices\n",
    "    crossval_results[model_name] = crossval_metrics['confusion_matrix']\n",
    "    bootstrap_results[model_name] = bootstrap_metrics['confusion_matrix']\n",
    "\n",
    "# 3. Build Dash App\n",
    "app = Dash(__name__)\n",
    "\n",
    "app.layout = html.Div([\n",
    "    html.H1(\"Titanic Classification Models\", style={'textAlign': 'center'}),\n",
    "\n",
    "    # Performance Metrics\n",
    "    html.H3(\"Performance Metrics\"),\n",
    "    dcc.Graph(figure=px.bar(\n",
    "        pd.DataFrame(metrics_results).reset_index(),\n",
    "        x='index', y=list(models.keys()),\n",
    "        barmode='group',\n",
    "        title=\"Performance Metrics for All Models\",\n",
    "        labels={'index': 'Metrics', 'value': 'Score'}\n",
    "    )),\n",
    "\n",
    "    html.H3(\"Confusion Matrices\"),\n",
    "    html.Div([\n",
    "        dcc.Graph(figure=px.imshow(confusion_matrices[model], text_auto=True, title=f\"{model}\", width=300, height=300))\n",
    "        for model in models.keys()\n",
    "    ], style={'display': 'flex', 'flexDirection': 'row'}),\n",
    "\n",
    "    # Cross-validation\n",
    "    html.H3(\"Cross-Validation Results\"),\n",
    "    dcc.Graph(figure=px.line(\n",
    "        x=list(range(1, 11)),\n",
    "        y=[crossval_results[model] for model in models.keys()],\n",
    "        title=\"Cross-Validation Accuracy (10-Fold)\",\n",
    "        labels={'x': 'Fold', 'y': 'Accuracy', 'color': 'Models'}\n",
    "    )),\n",
    "\n",
    "    # Bootstrap .632 Results\n",
    "    html.H3(\"Bootstrap .632 Results\"),\n",
    "    dcc.Graph(figure=px.bar(\n",
    "        x=list(models.keys()),\n",
    "        y=[bootstrap_results[model] for model in models.keys()],\n",
    "        title=\"Bootstrap .632 Accuracy\",\n",
    "        labels={'x': 'Model', 'y': 'Bootstrap Accuracy', 'color': 'Models'},\n",
    "        color=list(models.keys())\n",
    "    )),\n",
    "])\n",
    "\n",
    "# Run the app\n",
    "if __name__ == '__main__':\n",
    "    app.run_server(debug=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Version 2: This one works but it retrain dataset for performance, so actually it's wrong."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import base64\n",
    "import io\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import StratifiedKFold, train_test_split, cross_val_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier, plot_tree\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, f1_score, precision_score, recall_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.utils import resample\n",
    "import plotly.graph_objects as go\n",
    "import plotly.express as px\n",
    "from dash import Dash, dcc, html\n",
    "\n",
    "# 1. DATA PREPARATION\n",
    "df = pd.read_csv('titanic.csv')\n",
    "\n",
    "# Data Cleaning\n",
    "df = df.drop(columns=['PassengerId', 'Name', 'Ticket', 'Cabin'], errors='ignore')\n",
    "df['Age'] = np.floor(df['Age'])\n",
    "df['Age'].fillna(df['Age'].median(), inplace=True)\n",
    "df['Embarked'].fillna(df['Embarked'].mode()[0], inplace=True)\n",
    "df = pd.get_dummies(df, columns=['Sex', 'Embarked'], drop_first=True)\n",
    "\n",
    "# Features and target\n",
    "X = df.drop(columns=['Survived'])\n",
    "y = df['Survived']\n",
    "\n",
    "# Feature Scaling\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Models\n",
    "models = {\n",
    "    \"Logistic Regression\": LogisticRegression(max_iter=1000),\n",
    "    \"Decision Tree\": DecisionTreeClassifier(random_state=42),\n",
    "    \"KNN (k=3)\": KNeighborsClassifier(n_neighbors=3)\n",
    "}\n",
    "\n",
    "# Cross-validation and Bootstrap methods\n",
    "def cross_validate_method(model, X, y):\n",
    "    cv = StratifiedKFold(n_splits=10, shuffle=True, random_state=42)\n",
    "    return cross_val_score(model, X, y, cv=cv, scoring='accuracy')\n",
    "\n",
    "def bootstrap_632_method(model, X, y):\n",
    "    train_acc, test_acc = [], []\n",
    "    for _ in range(100):\n",
    "        X_boot, y_boot = resample(X, y, replace=True)\n",
    "        X_oob = X[~np.in1d(np.arange(X.shape[0]), X_boot.index)]\n",
    "        y_oob = y[~np.in1d(np.arange(y.shape[0]), X_boot.index)]\n",
    "        model.fit(X_boot, y_boot)\n",
    "        acc_train = accuracy_score(y_boot, model.predict(X_boot))\n",
    "        acc_test = accuracy_score(y_oob, model.predict(X_oob)) if len(y_oob) > 0 else 0\n",
    "        train_acc.append(acc_train)\n",
    "        test_acc.append(acc_test)\n",
    "    return np.mean(0.368 * np.array(train_acc) + 0.632 * np.array(test_acc))\n",
    "\n",
    "# Evaluate models\n",
    "metrics_results = {}\n",
    "confusion_matrices = {}\n",
    "crossval_results = {}\n",
    "bootstrap_results = {}\n",
    "\n",
    "for model_name, model in models.items():\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    \n",
    "    # Save metrics\n",
    "    metrics_results[model_name] = {\n",
    "        'Accuracy': accuracy_score(y_test, y_pred),\n",
    "        'Precision': precision_score(y_test, y_pred),\n",
    "        'Recall': recall_score(y_test, y_pred),\n",
    "        'F1-Score': f1_score(y_test, y_pred)\n",
    "    }\n",
    "    confusion_matrices[model_name] = confusion_matrix(y_test, y_pred)\n",
    "    crossval_results[model_name] = cross_validate_method(model, X, y)\n",
    "    bootstrap_results[model_name] = bootstrap_632_method(model, X, y)\n",
    "\n",
    "# 3. Build Dash App\n",
    "app = Dash(__name__)\n",
    "\n",
    "app.layout = html.Div([\n",
    "    html.H1(\"Titanic Classification Models\", style={'textAlign': 'center'}),\n",
    "\n",
    "    # Performance Metrics\n",
    "    html.H3(\"Performance Metrics\"),\n",
    "    dcc.Graph(figure=px.bar(\n",
    "        pd.DataFrame(metrics_results).reset_index(),\n",
    "        x='index', y=list(models.keys()),\n",
    "        barmode='group',\n",
    "        title=\"Performance Metrics for All Models\",\n",
    "        labels={'index': 'Metrics', 'value': 'Score'}\n",
    "    )),\n",
    "\n",
    "    # Confusion Matrices\n",
    "    html.H3(\"Confusion Matrices\"),\n",
    "    html.Div([\n",
    "        dcc.Graph(figure=px.imshow(confusion_matrices[model], text_auto=True, title=f\"Confusion Matrix: {model}\", width=300, height=300))\n",
    "        for model in models.keys()\n",
    "    ], style={'display': 'flex', 'flexDirection': 'row', 'flexWrap': 'wrap'}),\n",
    "    \n",
    "    # Cross-validation\n",
    "    html.H3(\"Cross-Validation Results\"),\n",
    "    dcc.Graph(figure=px.line(\n",
    "        x=list(range(1, 11)),\n",
    "        y=[crossval_results[model] for model in models.keys()],\n",
    "        title=\"Cross-Validation Accuracy (10-Fold)\",\n",
    "        labels={'x': 'Fold', 'y': 'Accuracy', 'color': 'Models'}\n",
    "    )),\n",
    "\n",
    "    # Bootstrap .632 Results\n",
    "    html.H3(\"Bootstrap .632 Results\"),\n",
    "    dcc.Graph(figure=px.bar(\n",
    "        x=list(models.keys()),\n",
    "        y=[bootstrap_results[model] for model in models.keys()],\n",
    "        title=\"Bootstrap .632 Accuracy\",\n",
    "        labels={'x': 'Model', 'y': 'Bootstrap Accuracy'},\n",
    "        color=list(models.keys()),\n",
    "        # color_discrete_sequence=px.colors.qualitative.Set1\n",
    "    )),\n",
    "\n",
    "\n",
    "])\n",
    "\n",
    "# Run the app\n",
    "if __name__ == '__main__':\n",
    "    app.run_server(debug=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Version 3 (huo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/y1/1tjj64zd37sdph6807cm97nh0000gn/T/ipykernel_16545/676778666.py:25: FutureWarning:\n",
      "\n",
      "A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "\n",
      "/var/folders/y1/1tjj64zd37sdph6807cm97nh0000gn/T/ipykernel_16545/676778666.py:27: FutureWarning:\n",
      "\n",
      "A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"100%\"\n",
       "            height=\"650\"\n",
       "            src=\"http://127.0.0.1:8050/\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "            \n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x330baaed0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier, plot_tree\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, f1_score, precision_score, recall_score, make_scorer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.utils import resample\n",
    "import plotly.express as px\n",
    "from dash import Dash, dcc, html\n",
    "import matplotlib.pyplot as plt\n",
    "import io\n",
    "import base64\n",
    "\n",
    "# 1. DATA PREPARATION\n",
    "df = pd.read_csv('titanic.csv')\n",
    "\n",
    "# Clean the data\n",
    "# These columns (PassengerId, Name, Ticket, Cabin) are not useful for predicting survival. They either contain unique identifiers or non-informative text data that do not contribute to the model's predictive power.\n",
    "df = df.drop(columns=['PassengerId', 'Name', 'Ticket', 'Cabin'], errors='ignore')\n",
    "# The 'Age' column has missing values. Using the median to fill these gaps is a common practice because it is robust to outliers and provides a central tendency measure.\n",
    "df['Age'] = np.floor(df['Age'])\n",
    "df['Age'].fillna(df['Age'].median(), inplace=True)\n",
    "# The 'Embarked' column has missing values. Filling them with the most frequent value.\n",
    "df['Embarked'].fillna(df['Embarked'].mode()[0], inplace=True)\n",
    "df = pd.get_dummies(df, columns=['Sex', 'Embarked'], drop_first=True)\n",
    "# Features and target: To prepare the data for modeling, we need to separate the features (input variables) from the target (output variable).\n",
    "X = df.drop(columns=['Survived'])\n",
    "y = df['Survived']\n",
    "\n",
    "# Feature Scaling\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Models\n",
    "models = {\n",
    "    \"Logistic Regression\": LogisticRegression(max_iter=1000),\n",
    "    \"Decision Tree\": DecisionTreeClassifier(random_state=42),\n",
    "    \"KNN (k=3)\": KNeighborsClassifier(n_neighbors=3)\n",
    "}\n",
    "\n",
    "def train_model(model, data_train):\n",
    "    \"\"\"Trains the model on the training data.\"\"\"\n",
    "    for data in data_train:\n",
    "        X_train, y_train = data[:2]  # Only use the first two values\n",
    "        model.fit(X_train, y_train)\n",
    "    return model\n",
    "\n",
    "def cross_validation_data(X, y):\n",
    "    \"\"\"Generates train-test splits for cross-validation.\"\"\"\n",
    "    cv = StratifiedKFold(n_splits=10, shuffle=True, random_state=42)\n",
    "\n",
    "    data_train = []\n",
    "    data_test = []\n",
    "\n",
    "    for train_idx, test_idx in cv.split(X, y):\n",
    "        X_train, X_test = X[train_idx], X[test_idx]\n",
    "        y_train, y_test = y[train_idx], y[test_idx]\n",
    "        data_train.append((X_train, y_train))\n",
    "        data_test.append((X_test, y_test))\n",
    "\n",
    "    return data_train, data_test\n",
    "\n",
    "# Cross-validation method\n",
    "def cross_validate_evaluation(trained_model, data_test):\n",
    "    \"\"\"Perform cross-validation and return metrics and confusion matrix.\"\"\"\n",
    "    metrics = {'accuracy': [], 'precision': [], 'recall': [], 'f1': []} # Store metrics for each fold\n",
    "    confusion_matrices = [] # Store confusion matrices for each fold\n",
    "\n",
    "    for X_test, y_test in data_test:\n",
    "        # predict the data\n",
    "        y_pred = trained_model.predict(X_test)\n",
    "        metrics['accuracy'].append(accuracy_score(y_test, y_pred))\n",
    "        metrics['precision'].append(precision_score(y_test, y_pred, average='weighted', zero_division=0))\n",
    "        metrics['recall'].append(recall_score(y_test, y_pred, average='weighted', zero_division=0))\n",
    "        metrics['f1'].append(f1_score(y_test, y_pred, average='weighted', zero_division=0))\n",
    "        confusion_matrices.append(confusion_matrix(y_test, y_pred))\n",
    "\n",
    "    avg_confusion_matrix = np.mean(confusion_matrices, axis=0)\n",
    "    avg_metrics = {key: np.mean(value) for key, value in metrics.items()}\n",
    "    \n",
    "    return { # return a dictionary containing the average metrics and the average confusion matrix.\n",
    "        **avg_metrics, # Unpack metrics\n",
    "        'confusion_matrix': avg_confusion_matrix\n",
    "        # 'decision_tree': dt_image\n",
    "    }\n",
    "\n",
    "def bootstrap_632_data(X, y):\n",
    "    \"\"\"Generates train-test splits for the Bootstrap .632 method.\"\"\"\n",
    "    data = []\n",
    "    for _ in range(100):  # Number of bootstrap iterations\n",
    "        X_boot, y_boot = resample(X, y, replace=True, n_samples=len(X), random_state=42)\n",
    "        \n",
    "        oob_indices = np.setdiff1d(np.arange(len(X)), np.unique(X_boot, return_index=True)[1])\n",
    "        X_oob, y_oob = X[oob_indices], y[oob_indices]\n",
    "        data.append((X_boot, y_boot, X_oob, y_oob, oob_indices))\n",
    "\n",
    "    return data\n",
    "\n",
    "def bootstrap_632_evaluation(trained_model, data):\n",
    "    train_metrics = {'accuracy': [], 'precision': [], 'recall': [], 'f1': []}\n",
    "    test_metrics = {'accuracy': [], 'precision': [], 'recall': [], 'f1': []}\n",
    "    confusion_matrices = []\n",
    "\n",
    "    for X_boot, y_boot, X_oob, y_oob, oob_indices in data:\n",
    "\n",
    "        y_pred_train = trained_model.predict(X_boot) \n",
    "\n",
    "        # Collect train metrics\n",
    "        train_metrics['accuracy'].append(accuracy_score(y_boot, y_pred_train))\n",
    "        train_metrics['precision'].append(precision_score(y_boot, y_pred_train, average='weighted', zero_division=0))\n",
    "        train_metrics['recall'].append(recall_score(y_boot, y_pred_train, average='weighted', zero_division=0))\n",
    "        train_metrics['f1'].append(f1_score(y_boot, y_pred_train, average='weighted', zero_division=0))\n",
    "\n",
    "        # Collect test (OOB) metrics if there are out-of-bag samples\n",
    "        if len(oob_indices) > 0:\n",
    "            y_pred_oob = trained_model.predict(X_oob)\n",
    "            test_metrics['accuracy'].append(accuracy_score(y_oob, y_pred_oob))\n",
    "            test_metrics['precision'].append(precision_score(y_oob, y_pred_oob, average='weighted', zero_division=0))\n",
    "            test_metrics['recall'].append(recall_score(y_oob, y_pred_oob, average='weighted', zero_division=0))\n",
    "            test_metrics['f1'].append(f1_score(y_oob, y_pred_oob, average='weighted', zero_division=0))\n",
    "            confusion_matrices.append(confusion_matrix(y_oob, y_pred_oob))\n",
    "\n",
    "    # Aggregate train and test metrics\n",
    "    train_metrics_mean = {metric: np.mean(train_metrics[metric]) for metric in train_metrics}\n",
    "    test_metrics_mean = {metric: np.mean(test_metrics[metric]) for metric in test_metrics}\n",
    "\n",
    "    # Combine metrics using the .632 formula\n",
    "    combined_metrics = {metric: 0.368 * train_metrics_mean[metric] + 0.632 * test_metrics_mean[metric] for metric in train_metrics}\n",
    "    avg_confusion_matrix = np.mean(confusion_matrices, axis=0) if confusion_matrices else None\n",
    "\n",
    "    combined_metrics['confusion_matrix'] = avg_confusion_matrix\n",
    "    return combined_metrics\n",
    "\n",
    "    \n",
    "# Bootstrap method\n",
    "def bootstrap_632_method(model, X, y):\n",
    "    \"\"\"Perform Bootstrap .632 method with extended metrics.\"\"\"\n",
    "    train_metrics = {'accuracy': [], 'precision': [], 'recall': [], 'f1': []}\n",
    "    test_metrics = {'accuracy': [], 'precision': [], 'recall': [], 'f1': []}\n",
    "    confusion_matrices = []\n",
    "\n",
    "    for _ in range(100):  # Number of bootstrap iterations\n",
    "        # Bootstrap sampling using resample\n",
    "        X_boot, y_boot = resample(X, y, replace=True, n_samples=len(X), random_state=42)\n",
    "        oob_indices = np.setdiff1d(np.arange(len(X)), np.unique(X_boot, return_index=True)[1])\n",
    "\n",
    "        X_oob, y_oob = X[oob_indices], y[oob_indices]\n",
    "\n",
    "        # Train the model on in-bag data\n",
    "        model.fit(X_boot, y_boot)\n",
    "        y_pred_train = model.predict(X_boot)\n",
    "\n",
    "        # Collect train metrics\n",
    "        train_metrics['accuracy'].append(accuracy_score(y_boot, y_pred_train))\n",
    "        train_metrics['precision'].append(precision_score(y_boot, y_pred_train, average='weighted', zero_division=0))\n",
    "        train_metrics['recall'].append(recall_score(y_boot, y_pred_train, average='weighted', zero_division=0))\n",
    "        train_metrics['f1'].append(f1_score(y_boot, y_pred_train, average='weighted', zero_division=0))\n",
    "\n",
    "        # Collect test (OOB) metrics if there are out-of-bag samples\n",
    "        if len(oob_indices) > 0:\n",
    "            y_pred_oob = model.predict(X_oob)\n",
    "            test_metrics['accuracy'].append(accuracy_score(y_oob, y_pred_oob))\n",
    "            test_metrics['precision'].append(precision_score(y_oob, y_pred_oob, average='weighted', zero_division=0))\n",
    "            test_metrics['recall'].append(recall_score(y_oob, y_pred_oob, average='weighted', zero_division=0))\n",
    "            test_metrics['f1'].append(f1_score(y_oob, y_pred_oob, average='weighted', zero_division=0))\n",
    "            confusion_matrices.append(confusion_matrix(y_oob, y_pred_oob))\n",
    "\n",
    "    # Aggregate train and test metrics\n",
    "    train_metrics_mean = {metric: np.mean(train_metrics[metric]) for metric in train_metrics}\n",
    "    test_metrics_mean = {metric: np.mean(test_metrics[metric]) for metric in test_metrics}\n",
    "\n",
    "    # Combine metrics using the .632 formula\n",
    "    combined_metrics = {metric: 0.368 * train_metrics_mean[metric] + 0.632 * test_metrics_mean[metric] for metric in train_metrics}\n",
    "    avg_confusion_matrix = np.mean(confusion_matrices, axis=0) if confusion_matrices else None\n",
    "\n",
    "    combined_metrics['confusion_matrix'] = avg_confusion_matrix\n",
    "    return combined_metrics\n",
    "\n",
    "# decision tree visualisieren\n",
    "def decision_tree_visualization_cv(trained_model, X):\n",
    "    \n",
    "    buf = io.BytesIO()\n",
    "    plt.figure(figsize=(100, 90))\n",
    "    plot_tree(trained_model, feature_names=X.columns, class_names=[\"Not Survived\", \"Survived\"], filled=True)\n",
    "    plt.savefig(buf, format=\"png\")\n",
    "    plt.close()\n",
    "    return base64.b64encode(buf.getbuffer()).decode(\"utf8\")\n",
    "\n",
    "def decision_tree_visualization_bs(trained_model, X):\n",
    "    \n",
    "    buf = io.BytesIO()\n",
    "    plt.figure(figsize=(100, 90))\n",
    "    plot_tree(trained_model, feature_names=X.columns, class_names=[\"Not Survived\", \"Survived\"], filled=True)\n",
    "    plt.savefig(buf, format=\"png\")\n",
    "    plt.close()\n",
    "    return base64.b64encode(buf.getbuffer()).decode(\"utf8\")\n",
    "\n",
    "# Evaluate models\n",
    "crossval_results = {}\n",
    "bootstrap_results = {}\n",
    "decision_tree_cv = {}\n",
    "decision_tree_bs = {}\n",
    "\n",
    "for model_name, model in models.items():\n",
    "    # Cross-validation\n",
    "    data_train_cv, data_test_cv = cross_validation_data(X_scaled, y)\n",
    "    trained_model_cv = train_model(model, data_train_cv)\n",
    "    crossval_results[model_name] = cross_validate_evaluation(trained_model_cv, data_test_cv)\n",
    "    \n",
    "    # Bootstrap\n",
    "    data_bs = bootstrap_632_data(X_scaled, y)\n",
    "    trained_model_bs = train_model(model, data_bs)\n",
    "    bootstrap_results[model_name] = bootstrap_632_evaluation(trained_model_bs, data_bs)\n",
    "\n",
    "    # Store decision tree visualization if the model is DecisionTreeClassifier\n",
    "    if isinstance(model, DecisionTreeClassifier):\n",
    "        decision_tree_cv = decision_tree_visualization_cv(trained_model_cv, X)\n",
    "        decision_tree_bs = decision_tree_visualization_bs(trained_model_bs, X)\n",
    "\n",
    "\n",
    "# 3. Build Dash App\n",
    "app = Dash(__name__)\n",
    "\n",
    "app.layout = html.Div([\n",
    "    html.H1(\"Titanic Classification Models Evaluation\", style={'textAlign': 'center'}),\n",
    "\n",
    "    # Metrics Comparison\n",
    "    html.H3(\"Cross-Validation Metrics Comparison\"),\n",
    "    dcc.Graph(figure=px.bar(\n",
    "        pd.DataFrame([\n",
    "            {'Metric': metric, 'Model': model, 'Score': crossval_results[model][metric]}\n",
    "            for model in models.keys()\n",
    "            for metric in ['accuracy', 'precision', 'recall', 'f1']\n",
    "        ]),\n",
    "        x='Metric', y='Score', color='Model', barmode='group',\n",
    "        title=\"Cross-Validation Metrics Comparison\",\n",
    "        labels={'Metric': 'Metrics', 'Score': 'Score'}\n",
    "    )),\n",
    "\n",
    "    html.H3(\"Bootstrap Metrics Comparison\"),\n",
    "    dcc.Graph(figure=px.bar(\n",
    "        pd.DataFrame([\n",
    "            {'Metric': metric, 'Model': model, 'Score': bootstrap_results[model][metric]}\n",
    "            for model in models.keys()\n",
    "            for metric in ['accuracy', 'precision', 'recall', 'f1']\n",
    "        ]),\n",
    "        x='Metric', y='Score', color='Model', barmode='group',\n",
    "        title=\"Bootstrap Metrics Comparison\",\n",
    "        labels={'Metric': 'Metrics', 'Score': 'Score'}\n",
    "    )),\n",
    "\n",
    "    # Confusion Matrices\n",
    "    html.H3(\"Confusion Matrices: Cross-Validation\"),\n",
    "    html.Div([\n",
    "        html.Div([\n",
    "            dcc.Graph(figure=px.imshow(crossval_results[model]['confusion_matrix'], text_auto=True,\n",
    "                                        title=f\"{model}\",\n",
    "                                        color_continuous_scale='Blues').update_layout(\n",
    "                                            autosize=False,\n",
    "                                            width=300,\n",
    "                                            height=300\n",
    "                                        ))\n",
    "        ], style={'margin': '10px'})\n",
    "        for model in models.keys()\n",
    "    ], style={'display': 'flex', 'flexDirection': 'row', 'flexWrap': 'wrap'}),\n",
    "    \n",
    "    html.H3(\"Confusion Matrices: Bootstrap\"),\n",
    "    html.Div([\n",
    "        html.Div([\n",
    "            dcc.Graph(figure=px.imshow(bootstrap_results[model]['confusion_matrix'], text_auto=True,\n",
    "                                        title=f\"{model}\",\n",
    "                                        color_continuous_scale='Reds').update_layout(\n",
    "                                            autosize=False,\n",
    "                                            width=300,\n",
    "                                            height=300\n",
    "                                        ))\n",
    "        ], style={'margin': '10px'})\n",
    "        for model in models.keys()\n",
    "    ], style={'display': 'flex', 'flexDirection': 'row', 'flexWrap': 'wrap'}),\n",
    "\n",
    "    # Decision Tree visualisierung\n",
    "    html.H3(\"Decision Tree(Crossvalidation)\"),\n",
    "    html.Img(src=\"data:image/png;base64,{}\".format(decision_tree_cv), style={'width': '100%', 'height': 'auto'}),\n",
    "    html.H3(\"Decision Tree(Bootstrap)\"),\n",
    "    html.Img(src=\"data:image/png;base64,{}\".format(decision_tree_bs), style={'width': '100%', 'height': 'auto'}),\n",
    "\n",
    "])\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    app.run_server(debug=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Version4\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"100%\"\n",
       "            height=\"650\"\n",
       "            src=\"http://127.0.0.1:8050/\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "            \n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x31e747bf0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "import plotly.graph_objects as go\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, f1_score, precision_score, recall_score, make_scorer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.utils import resample\n",
    "from sklearn.tree import DecisionTreeClassifier, plot_tree\n",
    "import plotly.express as px\n",
    "from dash import Dash, dcc, html\n",
    "import io\n",
    "import base64\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# 1. DATA PREPARATION\n",
    "df = pd.read_csv('titanic.csv')\n",
    "\n",
    "# Clean the data\n",
    "df = df.drop(columns=['PassengerId', 'Name', 'Ticket', 'Cabin'], errors='ignore')\n",
    "df['Age'] = np.floor(df['Age'])\n",
    "df['Age'] = df['Age'].fillna(df['Age'].median())\n",
    "df['Embarked'] = df['Embarked'].fillna(df['Embarked'].mode()[0])\n",
    "df = pd.get_dummies(df, columns=['Sex', 'Embarked'], drop_first=True)\n",
    "X = df.drop(columns=['Survived'])\n",
    "y = df['Survived']\n",
    "\n",
    "# Feature Scaling\n",
    "#scaler = StandardScaler()\n",
    "#X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Models array (2D array for Cross-Validation and Bootstrap)\n",
    "models = {\n",
    "    \"Logistic Regression\": [LogisticRegression(max_iter=1000), LogisticRegression(max_iter=1000)],\n",
    "    \"Decision Tree\": [DecisionTreeClassifier(random_state=42), DecisionTreeClassifier(random_state=42)],\n",
    "    \"KNN (k=3)\": [KNeighborsClassifier(n_neighbors=3), KNeighborsClassifier(n_neighbors=3)],\n",
    "}\n",
    "\n",
    "# Cross-validation method\n",
    "def cross_validate_method(model, X, y):\n",
    "    cv = StratifiedKFold(n_splits=10, shuffle=True, random_state=42)\n",
    "    metrics = {'accuracy': [], 'precision': [], 'recall': [], 'f1': []}\n",
    "    confusion_matrices = []\n",
    "\n",
    "    for train_idx, test_idx in cv.split(X, y):\n",
    "        X_train, X_test = X.iloc[train_idx], X.iloc[test_idx] if isinstance(X, pd.DataFrame) else (X[train_idx], X[test_idx])\n",
    "        y_train, y_test = y.iloc[train_idx], y.iloc[test_idx] if isinstance(y, pd.Series) else (y[train_idx], y[test_idx])\n",
    "\n",
    "        model.fit(X_train, y_train)\n",
    "        y_pred = model.predict(X_test)\n",
    "        metrics['accuracy'].append(accuracy_score(y[test_idx], y_pred))\n",
    "        metrics['precision'].append(precision_score(y[test_idx], y_pred, average='weighted', zero_division=0))\n",
    "        metrics['recall'].append(recall_score(y[test_idx], y_pred, average='weighted', zero_division=0))\n",
    "        metrics['f1'].append(f1_score(y[test_idx], y_pred, average='weighted', zero_division=0))\n",
    "        confusion_matrices.append(confusion_matrix(y[test_idx], y_pred))\n",
    "\n",
    "    avg_confusion_matrix = np.mean(confusion_matrices, axis=0)\n",
    "    avg_metrics = {key: np.mean(value) for key, value in metrics.items()}\n",
    "\n",
    "    return { \n",
    "        **avg_metrics,\n",
    "        'confusion_matrix': avg_confusion_matrix\n",
    "    }\n",
    "\n",
    "# Bootstrap method\n",
    "def bootstrap_632_method(model, X, y):\n",
    "    train_metrics = {'accuracy': [], 'precision': [], 'recall': [], 'f1': []}\n",
    "    test_metrics = {'accuracy': [], 'precision': [], 'recall': [], 'f1': []}\n",
    "    confusion_matrices = []\n",
    "\n",
    "    for _ in range(100):  \n",
    "        # Resample for bootstrap\n",
    "        X_boot, y_boot = resample(X, y, replace=True, n_samples=len(X), random_state=42)\n",
    "\n",
    "        # Berechne die Out-of-Bag (OOB)-Indizes\n",
    "        oob_indices = np.setdiff1d(np.arange(len(X)), np.unique(X_boot, return_index=True)[1])\n",
    "\n",
    "        # Verwende .iloc[] für pandas DataFrame/Series \n",
    "        X_oob = X.iloc[oob_indices] if isinstance(X, pd.DataFrame) else X[oob_indices]\n",
    "        y_oob = y.iloc[oob_indices] if isinstance(y, pd.Series) else y[oob_indices]\n",
    "\n",
    "        # Trainiere das Modell mit den Bootstrap-Daten\n",
    "        model.fit(X_boot, y_boot)\n",
    "        y_pred_train = model.predict(X_boot)\n",
    "\n",
    "        train_metrics['accuracy'].append(accuracy_score(y_boot, y_pred_train))\n",
    "        train_metrics['precision'].append(precision_score(y_boot, y_pred_train, average='weighted', zero_division=0))\n",
    "        train_metrics['recall'].append(recall_score(y_boot, y_pred_train, average='weighted', zero_division=0))\n",
    "        train_metrics['f1'].append(f1_score(y_boot, y_pred_train, average='weighted', zero_division=0))\n",
    "\n",
    "        if len(oob_indices) > 0:\n",
    "            y_pred_oob = model.predict(X_oob)\n",
    "            test_metrics['accuracy'].append(accuracy_score(y_oob, y_pred_oob))\n",
    "            test_metrics['precision'].append(precision_score(y_oob, y_pred_oob, average='weighted', zero_division=0))\n",
    "            test_metrics['recall'].append(recall_score(y_oob, y_pred_oob, average='weighted', zero_division=0))\n",
    "            test_metrics['f1'].append(f1_score(y_oob, y_pred_oob, average='weighted', zero_division=0))\n",
    "            confusion_matrices.append(confusion_matrix(y_oob, y_pred_oob))\n",
    "            \n",
    "    train_metrics_mean = {metric: np.mean(train_metrics[metric]) for metric in train_metrics}\n",
    "    test_metrics_mean = {metric: np.mean(test_metrics[metric]) for metric in test_metrics}\n",
    "\n",
    "    combined_metrics = {metric: 0.368 * train_metrics_mean[metric] + 0.632 * test_metrics_mean[metric] for metric in train_metrics}\n",
    "    avg_confusion_matrix = np.mean(confusion_matrices, axis=0) if confusion_matrices else None\n",
    "\n",
    "    combined_metrics['confusion_matrix'] = avg_confusion_matrix\n",
    "    return combined_metrics\n",
    "\n",
    "# Visualize Decision Tree (cross-validation and bootstrap)\n",
    "def decision_tree_visualization(model, X, y):\n",
    "    # Visualize the Decision Tree\n",
    "    buf = io.BytesIO()\n",
    "    plt.figure(figsize=(100, 90))\n",
    "    plot_tree(model, feature_names=X.columns, class_names=[\"Not Survived\", \"Survived\"], filled=True)\n",
    "    plt.savefig(buf, format=\"png\")\n",
    "    plt.close()\n",
    "    return base64.b64encode(buf.getbuffer()).decode(\"utf8\")\n",
    "\n",
    "# Evaluate models\n",
    "crossval_results = {}\n",
    "bootstrap_results = {}\n",
    "\n",
    "for model_name, model_pair in models.items():\n",
    "    crossval_results[model_name] = cross_validate_method(model_pair[0], X, y)\n",
    "    bootstrap_results[model_name] = bootstrap_632_method(model_pair[1], X, y)\n",
    "\n",
    "\n",
    "# For decision tree visualization (cross-validation and bootstrap)\n",
    "decision_tree_cv = decision_tree_visualization(models[\"Decision Tree\"][0], X, y)\n",
    "decision_tree_bootstrap = decision_tree_visualization(models[\"Decision Tree\"][1], X, y)\n",
    "\n",
    "# 3. Build Dash App\n",
    "app = Dash(__name__)\n",
    "\n",
    "app.layout = html.Div([\n",
    "    html.H1(\"Titanic Classification Models Evaluation\", style={'textAlign': 'center'}),\n",
    "\n",
    "    # Metrics Comparison\n",
    "    html.H3(\"Cross-Validation Metrics Comparison\"),\n",
    "    dcc.Graph(figure=go.Figure(\n",
    "        data=[go.Bar(\n",
    "            x=['accuracy', 'precision', 'recall', 'f1'],\n",
    "            y=[crossval_results[model][metric] for metric in ['accuracy', 'precision', 'recall', 'f1']],\n",
    "            name=model\n",
    "        ) for model in models.keys()],\n",
    "        layout=go.Layout(\n",
    "            barmode='group',\n",
    "            title=\"Cross-Validation Metrics Comparison\",\n",
    "            xaxis=dict(title=\"Metric\"),\n",
    "            yaxis=dict(title=\"Score\")\n",
    "        )\n",
    "    )),\n",
    "    \n",
    "    html.H3(\"Bootstrap Metrics Comparison\"),\n",
    "    dcc.Graph(figure=go.Figure(\n",
    "        data=[go.Bar(\n",
    "            x=['accuracy', 'precision', 'recall', 'f1'],\n",
    "            y=[bootstrap_results[model][metric] for metric in ['accuracy', 'precision', 'recall', 'f1']],\n",
    "            name=model\n",
    "        ) for model in models.keys()],\n",
    "        layout=go.Layout(\n",
    "            barmode='group',\n",
    "            title=\"Bootstrap Metrics Comparison\",\n",
    "            xaxis=dict(title=\"Metric\"),\n",
    "            yaxis=dict(title=\"Score\")\n",
    "        )\n",
    "    )),\n",
    "    html.H3(\"Confusion Matrices: Cross-Validation\"),\n",
    "    html.Div([\n",
    "        html.Div([\n",
    "            dcc.Graph(figure=px.imshow(crossval_results[model]['confusion_matrix'], text_auto=True,\n",
    "                                        title=f\"{model}\",\n",
    "                                        color_continuous_scale='Blues').update_layout(\n",
    "                                            autosize=False,\n",
    "                                            width=300,\n",
    "                                            height=300\n",
    "                                        ))\n",
    "        ], style={'margin': '10px'})\n",
    "        for model in models.keys()\n",
    "    ], style={'display': 'flex', 'flexDirection': 'row', 'flexWrap': 'wrap'}),\n",
    "\n",
    "    html.H3(\"Confusion Matrices: Bootstrap\"),\n",
    "    html.Div([\n",
    "        html.Div([\n",
    "            dcc.Graph(figure=px.imshow(bootstrap_results[model]['confusion_matrix'], text_auto=True,\n",
    "                                        title=f\"{model}\",\n",
    "                                        color_continuous_scale='Reds').update_layout(\n",
    "                                            autosize=False,\n",
    "                                            width=300,\n",
    "                                            height=300\n",
    "                                        ))\n",
    "        ], style={'margin': '10px'})\n",
    "        for model in models.keys()\n",
    "    ], style={'display': 'flex', 'flexDirection': 'row', 'flexWrap': 'wrap'}),\n",
    "\n",
    "    \n",
    "\n",
    "    # Decision Tree visualizations\n",
    "    html.H3(\"Decision Tree (Cross-Validation)\"),\n",
    "    html.Img(src=\"data:image/png;base64,{}\".format(decision_tree_cv), style={'width': '100%', 'height': 'auto'}),\n",
    "\n",
    "    html.H3(\"Decision Tree (Bootstrap)\"),\n",
    "    html.Img(src=\"data:image/png;base64,{}\".format(decision_tree_bootstrap), style={'width': '100%', 'height': 'auto'})\n",
    "])\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    app.run_server(debug=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
